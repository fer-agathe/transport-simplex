# German Credit Dataset {#sec-german-credit}

:::{.callout-note}

## Objectives


We apply the two methods presented in [Chapter -@sec-toy-dataset] to build counterfactuals for a categorical feature.

Recall that the first method, outlined in [Algorithm 1.1](application-toydataset.qmd#alg-1), consists in using Gaussian optimal transport based on an alternative representation of the probability vector (in the Euclidean space $\mathbb{R}^{d-1}$).

The second method uses transport and matching directly within the simplex $\mathcal{S}_d$ using an appropriate cost function.

For a categorical feature $\mathbf{x}_j$ with $d$ categories, we fit a model $\widehat{m}(\mathbf{x}_j|\mathbf{x}_{-j})$ using a multinomial loss, yielding predicted scores $\widehat{\mathbf{x}}_j\in\mathcal{S}_d$. We then apply [Algorithm 1.1](application-toydataset.qmd#alg-1) (first method) to transport $\widehat{\mathbf{x}}_j|s=0$ to the counterfactual $\widehat{\mathbf{x}}_j|s=1$ in $\mathcal{S}_d$. Then, we apply [Algorithm 1.2](application-toydataset.qmd#alg-2) (second method) to build the counterfactuals using matching in $\mathcal{S}_d$.

We illustrate the procedure with the German Credit dataset (@misc_german). Specifically, we transport the variable `Purpose` ($\mathbf{x}_j$), a factor with ten levels indicating loan purposes, conditioned on gender ($s$), encoded as a binary variable.

To facilitate visualization using ternary graphs, we recode `Purpose` into three categories: "cars," "equipment," and "other." Our goal is to construct a counterfactual loan purpose assuming individuals had a different gender.


:::

```{r, echo=FALSE}
# in the console, to be able to display pseudocodes:
# quarto add leovan/quarto-pseudocode

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    if (grep(x = color, "^#")) {
      color <- deparse(substitute(color))
    }
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}

col_0 <- "red"
col_1 <- "blue"
```


```{r}
library(tidyverse)
library(devtools)
library(ggtern)
# load the functions from our package to perform optimal transport on 
# compositional data
load_all("../../")
```

```{r define-theme_paper}
#| code-fold: true
#| code-summary: Definition of a ggplot2 theme.
#' Theme for ggplot2
#'
#' @param ... arguments passed to the theme function
#' @export
#' @importFrom ggplot2 element_rect element_text element_blank element_line unit
#'   rel
theme_paper <- function(...) {
  theme(
    text = element_text(family = "Times New Roman"),
    plot.background = element_rect(fill = "transparent", color = NA),
    legend.text = element_text(size = rel(1.1)),
    legend.title = element_text(size = rel(1.1)),
    legend.background = element_rect(
     fill = "transparent", linetype="solid", colour ="black"),
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.box = "vertical",
    legend.key = element_blank(),
    panel.spacing = unit(1, "lines"),
    plot.title = element_text(hjust = 0, size = rel(1.3), face = "bold"),
    plot.title.position = "plot",
    strip.background = element_rect(fill = NA, colour = NA),
    strip.text = element_text(size = rel(1.1))
  )
}
```


## The Dataset

The dataset is available in the [{fairml}](https://cran.r-project.org/web/packages/fairml/index.html) package.

```{r load-german.credit}
data(german.credit, package = "fairml")
str(german.credit)
```

We want to transport the `Purpose` variable, which has 10 different levels in the dataset. To be able to visualize the transport, we merge some of the classes together to obtain three main purposes: cars, equipment, and other.

```{r redefine-purpose}
german.credit <- 
  german.credit |> 
  mutate(
    Purpose = case_when(
      Purpose %in% c("car (new)","car (used)") ~ "cars",
      Purpose %in% c(
        "domestic appliances", "furniture / equipment", "radio / television"
      ) ~ "equipment",
      TRUE ~ "other"
    ) |> as.factor()
  )
```


## Estimation of Scores

We fit four models, $\widehat{m}^{(k)}(\mathbf{x}_j|\mathbf{x}_{-j})$, using a multinomial loss, yielding predicted scores $\widehat{\mathbf{x}}_{j}^{(k)}\in\mathcal{S}_d$ for $k \in \{1,2,3,4\}$. The sensitive attribute $s$ and the credit risk variable (Bad/Good)--typically the target variable in this dataset but not in our case--are excluded from estimation. The four models are:

- **GAM-MLR (1):** A multinomial model with splines for three continuous variables.
- **GAM-MLR (2):** A multinomial model with splines for three continuous variables and seven additional predictors.
- **Random Forest:** A classifier using all available variables.
- **Gradient Boosting Model:** A GBM trained on all available variables.

The estimation of GAM-MLR (1):
```{r train-gam-mlr-1}
library(splines)
require(nnet)
set.seed(123)
model_glm_1 <- multinom(
  Purpose ~ bs(Credit_amount) + bs(Age) + bs(Duration), 
  data = german.credit
)
```

The estimation of GAM-MLR (2):
```{r train-gam-mlr-2, message=FALSE, warning=FALSE}
model_glm_2 <- multinom(
  Purpose ~ bs(Credit_amount) + bs(Age) + bs(Duration) +
    Present_employment_since + Savings_bonds + Property + Account_status + 
    Credit_history + Resident_since + Job + Housing,
  data = german.credit,
)
```

The estimation of the random forest:
```{r train-rf, message=FALSE, warning=FALSE}
library(randomForest)
model_rf <- randomForest(Purpose ~ ., data = german.credit[, -c(20,21)])
```

The estimation of the gradient boosting model:
```{r train-gbm, message=FALSE, warning=FALSE}
library(gbm)
library(caret)
model_gbm <- gbm(
  Purpose ~ .,
  data = german.credit[, -c(20,21)],
  distribution = "multinomial",
  cv.folds = 10,
  shrinkage = .01,
  n.minobsinnode = 10,
  n.trees = 2000
)
```


Now that we have estimated the four models, we can extract the estimates scores $\widehat{\mathbf{x}}_{j}^{(k)}\in\mathcal{S}_d$:

```{r evaluate-scores}
scores_glm_1 <- predict(model_glm_1, type = "probs")
scores_glm_2 <- predict(model_glm_2, type = "probs")
scores_rf <- predict(model_rf, type = "prob")
scores_gbm <- predict.gbm(
  object = model_gbm,
  newdata = german.credit[, -c(20,21)],
  n.trees = 2000,
  type = "response"
)
scores_gbm <- scores_gbm[ , , 1]
```


Let us have a look at the predicted scores of four individuals (2 women, 2 men), for each model. 
```{r tb_four_indiv}
#| code-fold: true
#| code-summary: Codes to create the Table.
#| tbl-cap: Mappings from the `purpose` categorical variable $x$ to the compositional one $\tilde{\mathbf{x}}$, for four individuals of the dataset.
#| label: tbl-four-indiv
print_highlighted_obs <- function(scores) {
  ind_highlight <- c(2, 5, 13, 8)
  tb <- german.credit |>
    select(Purpose, Gender) |> 
    bind_cols(scores) |> 
    slice(ind_highlight)
  row.names(tb) <- NULL
  tb
}

tb_four_indiv <- 
  print_highlighted_obs(scores_glm_1) |> mutate(model = "glm_1") |> 
  bind_rows(
    print_highlighted_obs(scores_glm_2) |> mutate(model = "glm_2")
  ) |> 
  bind_rows(
    print_highlighted_obs(scores_rf) |> mutate(model = "rf")
  ) |> 
  bind_rows(
    print_highlighted_obs(scores_gbm) |> mutate(model = "gbm")
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("glm_1", "glm_2", "rf", "gbm"),
      labels = c("GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
                 "Gradient Boosting Model")
    )
  ) |> 
  relocate(model, .before = Purpose)

tb_four_indiv |> select(-model) |> 
  kableExtra::kbl(
    booktabs = TRUE, digits = 4,
  ) |> 
  kableExtra::kable_paper() |> 
  kableExtra::add_header_above(c(" " = 2, "Predicted Scores" = 3)) |> 
  kableExtra::pack_rows(index = table(tb_four_indiv$model))
```


## Optimal Transport in the Euclidean Representation

We can now apply [Algorithm 1.1](application-toydataset.qmd#alg-1) with a Gaussian mapping in an Euclidean representation space to transport from observed $\widehat{\mathbf{x}}_j|s=0$ (scores for women) to counterfactual $\widehat{\mathbf{x}}_j|s=1$ (scores for men), in $\mathcal{S}_d$.


Let us isolate women and men:
```{r define-ind_0}
ind_0 <- which(german.credit$Gender == "Female")
ind_1 <- which(german.credit$Gender == "Male")
```

Then, we create matrices for each model with the predicted scores for each category, i.e., the representation of the categorical variable `Purpose` in the unit simplex.
```{r define-X0_glm_1}
# GAM-MLR(1)
X0_glm_1 <- scores_glm_1[ind_0,]
X1_glm_1 <- scores_glm_1[ind_1,]
# GAM-MLR(2)
X0_glm_2 <- scores_glm_2[ind_0,]
X1_glm_2 <- scores_glm_2[ind_1,]
# RF
X0_rf <- scores_rf[ind_0,]
X1_rf <- scores_rf[ind_1,]
# GBM
X0_gbm <- scores_gbm[ind_0,]
X1_gbm <- scores_gbm[ind_1,]
```

Then, we can apply [Algorithm 1.1](application-toydataset.qmd#alg-1) to each set of predicted scores. We use the clr transform and Gaussian OT:
```{r apply-alg-1}
transp_glm_1 <- transport_simplex(X0 = X0_glm_1, X1 = X1_glm_1, n_interp = 31)
transp_glm_2 <- transport_simplex(X0 = X0_glm_2, X1 = X1_glm_2, n_interp = 31)
transp_rf <- transport_simplex(X0 = X0_rf, X1 = X1_rf, n_interp = 31)
transp_gbm <- transport_simplex(X0 = X0_rf, X1 = X1_gbm, n_interp = 31)
```

We can then have a look at the percentage of each purpose category in the initial dataset, compare it with the average predicted score of each category for each model, and with the average of the transported predicted score for women.


```{r}
#| code-fold: true
#| code-summary: Codes to create the Table.
#| tbl-cap: Optimal transport using the $\operatorname{clr}$ transformation, and Gaussian optimal transports, on the \texttt{purpose} scores in the \texttt{German Credit} database, with two logistic GAM models to predict scores, a random forest, and a boosting model. For observed values, the observed proportions of purpose categories are reported by gender. Then, for each model, the average of predicted scores by gender for each categories are shown (Composition). Lastly, the average of transported predicted scores for women are reported (Transported).
#| label: tbl-german-counterfactuals

# Proportions of each purpose level by gender in the dataset
prop_purposes <- 
  german.credit |> 
  count(Purpose, Gender) |> 
  group_by(Gender) |> 
  mutate(prop = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = Purpose, values_from = prop) |> 
  mutate(type = "Categorical")

get_table_pred_transp <- function(scores, transp_scores) {
  # Average predicted scores for each purpose level by gender
  mean_scores_by_gender <- 
    german.credit |> 
    select(Purpose, Gender) |> 
    bind_cols(scores) |> 
    group_by(Gender) |> 
    summarise(across(colnames(!!scores), ~mean(.x))) |> 
    mutate(type = "Composition")
  
  # Average predicted transported score of women for each purpose level
  mean_transp_scores_women <- colMeans(transp_scores) |> 
    as_tibble_row() |>
    mutate(type = "Transported", Gender = "Female -> Male")
  
  mean_scores_by_gender |> 
    bind_rows(mean_transp_scores_women)
}

tb_pred_transp_mean <-
  prop_purposes |> mutate(model = "obs") |> 
  bind_rows(
    get_table_pred_transp(scores_glm_1, transp_glm_1) |> mutate(model = "glm_1")
  ) |> 
  bind_rows(
    get_table_pred_transp(scores_glm_2, transp_glm_2) |> mutate(model = "glm_2")
  ) |> 
  bind_rows(
    get_table_pred_transp(scores_rf, transp_rf) |> mutate(model = "rf")
  ) |> 
  bind_rows(
    get_table_pred_transp(scores_gbm, transp_gbm) |> mutate(model = "gbm")
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  ) |> 
  relocate(model, .before = Gender) |> 
  relocate(type, .after = model)

tb_pred_transp_mean |> select(-model) |> 
  kableExtra::kbl(
  booktabs = TRUE, digits = 4,
) |> 
  kableExtra::kable_paper() |> 
  kableExtra::add_header_above(c(" " = 2, "Purposes" = 3)) |> 
  kableExtra::pack_rows(index = table(tb_pred_transp_mean$model))

```

### Visualization of Transported Categories

We can then show the counterfactuals on a ternary plot, and graph the displacement interpolation when generationg from the factual (women) to the counterfactuals (men).

First, we format the paths:

```{r}
transp_glm_1_path <- interpolated(transp_glm_1) |> 
  list_rbind(names_to = "id_obs")
transp_glm_2_path <- interpolated(transp_glm_2) |> 
  list_rbind(names_to = "id_obs")
transp_rf_path <- interpolated(transp_rf) |> 
  list_rbind(names_to = "id_obs")
transp_gbm_path <- interpolated(transp_gbm) |> 
  list_rbind(names_to = "id_obs")
```

Then, we can show, for each model, the representation in the simplex of the categorical variable `Purpose`, by gender (`r colorize("women in red", "red")` and `r colorize("men in blue", "blue")`), as well as the displacement interpolation.

```{r}
#| code-fold: true
#| code-summary: Codes to create the Figure.
#| fig-cap: Optimal Transport using clr transform. Points in <span style="color:red;">red</span> are compositions for <span style="color:red;">women</span>, whereas points in <span style="color:blue;">blue</span> are compositions for <span style="color:blue;">men</span>. The lines indicate the displacement interpolation when generating counterfactuals.
#| label: fig-ternary-displacement
#| fig-height: 8
#| fig-width: 8
scores_all <- as_tibble(scores_glm_1) |> 
  mutate(Gender = german.credit$Gender, model = "glm_1") |> 
  bind_rows(
    as_tibble(scores_glm_2) |> 
      mutate(Gender = german.credit$Gender, model = "glm_2")
  ) |> 
  bind_rows(
    as_tibble(as.data.frame(scores_rf)) |> 
      mutate(Gender = german.credit$Gender, model = "rf")
  ) |> 
  bind_rows(
    as_tibble(scores_gbm) |> 
      mutate(Gender = german.credit$Gender, model = "gbm")
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

transp_all_path <- 
  transp_glm_1_path |> 
  mutate(
    Gender = factor("Female", levels = levels(german.credit$Gender)),
    model = "glm_1"
  ) |> 
  bind_rows(
    transp_glm_2_path |> 
      mutate(
        Gender = factor("Female", levels = levels(german.credit$Gender)),
        model = "glm_2"
      ) 
  ) |> 
  bind_rows(
    transp_rf_path |> 
      mutate(
        Gender = factor("Female", levels = levels(german.credit$Gender)),
        model = "rf"
      ) 
  ) |> 
  bind_rows(
    transp_gbm_path |> 
      mutate(
        Gender = factor("Female", levels = levels(german.credit$Gender)),
        model = "gbm"
      ) 
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

ggtern(
  data = scores_all,
  mapping = aes(x = cars, y = other, z = equipment, colour = Gender)
) +
  geom_point(size = .1) +
  geom_line(
    data = transp_all_path, 
    linewidth = .2, alpha = .8,
    mapping = aes(group = id_obs)
  ) +
  scale_colour_manual(values = c("Female" = "red", "Male" = "blue"), guide = "none") +
  facet_wrap(~model) +
  labs(x = "Cars", y = "Other", z = "Equip.") +
  theme_paper() +
  theme(
    tern.axis.arrow.show = TRUE,
    tern.axis.arrow.sep = .13,
    tern.axis.vshift = .05,
  ) +
  theme_hidetitles()
```

## Optimal Transport in $\mathcal{S}_3$

Let us now use [Algorithm 1.2](application-toydataset.qmd#alg-2) to create counterfactuals using matching in $\mathcal{S}_3$.

To that end, we use the `wasserstein_simplex()`{.R} function from our package.

:::{.callout-warning}

#### Note

The codes are a bit long to run (about 40 seconds for each model on a 2023 MacBook Pro with an M2 chip). Here, we load results saved from a previously evaluated code.

:::

```{r define-mapping_glm_1, eval=FALSE}
mapping_glm_1 <- wasserstein_simplex(X0_glm_1, X1_glm_1)
mapping_glm_2 <- wasserstein_simplex(X0_glm_2, X1_glm_2)
mapping_rf <- wasserstein_simplex(X0_rf, X1_rf)
mapping_gbm <- wasserstein_simplex(X0_gbm, X1_gbm)

if (!dir.exists("../output/")) dir.create("../output/", recursive = TRUE)
save(
  mapping_glm_1, mapping_glm_2, mapping_rf, mapping_gbm,
  file = "../output/matching_german.rda"
)
```

```{r}
load("../output/matching_german.rda")
```


We extract the estimated weights for all the individuals:

```{r define-M0_glm_1}
M0_glm_1 <- mapping_glm_1$plan * nrow(X0_glm_1)
M0_glm_2 <- mapping_glm_2$plan * nrow(X0_glm_2)
M0_rf <- mapping_rf$plan * nrow(X0_rf)
M0_gbm <- mapping_gbm$plan * nrow(X0_gbm)
```

Let us focus on a single individual $x_{0,i}=$:
```{r define-indiv-to-lookup}
i <- 34
```

For each model, we extract the representation of the `Purpose` characteristic in the simplex (i.e., the predicted scores by the $k$-th model, $\widehat{m}^{(k)}(\mathbf{x}_j|\mathbf{x}_{-j})$). Let us denote this composition as $\mathbf{x}_{0,i}^{(k)}$
```{r define-indiv_i_glm_1}
indiv_i_glm_1 <- X0_glm_1[i, ]
indiv_i_glm_2 <- X0_glm_2[i, ]
indiv_i_rf <- X0_rf[i, ]
indiv_i_gbm <- X0_gbm[i, ]
```

We then extract the weights $\mathbf{P}^{\star(k)}_i$:
```{r define-weights_i_glm_1}
weights_i_glm_1 <- M0_glm_1[i, ]
weights_i_glm_2 <- M0_glm_2[i, ]
weights_i_rf <- M0_rf[i, ]
weights_i_gbm <- M0_gbm[i, ]
```

Lastly, we compute, for our individual, its counterfactual $T^\star(\mathbf{x}_{0,i})$, by simply computing the weighted average of the characteristics of the individuals from the other group.
```{r define-cfact_i_glm_1}
cfact_i_glm_1 <- weights_i_glm_1 %*% X1_glm_1
cfact_i_glm_2 <- weights_i_glm_2 %*% X1_glm_2
cfact_i_rf <- weights_i_rf %*% X1_rf
cfact_i_gbm <- weights_i_gbm %*% X1_gbm
```

We can then plot (@fig-ternary-matching) the representation of the woman of interest obtained for each model (`r colorize("red dot", "red")`), and its counterfactual obtained by matching (`r colorize("blue dot", "blue")`). We also plot, on the ternary plot, all the `r colorize("women", "red")` and `r colorize("men", "blue")`. The size of the dots for `r colorize("men", "blue")` is proportional to the weights corresponding to the `r colorize("women of interest", "red")`.





```{r}
#| code-fold: true
#| code-summary: Codes to create the Figure.
#| fig-cap: Empirical matching of a <span style="color:red;">woman</span> $\mathbf{x}_{0,i}^{(k)}$ (<span style="color:red;">big red dot</span>) with <span style="color:blue;">men</span> (<span style="color:blue;">blue dots</span>). The Size of blue dots are proportional to the weights $\mathbf{P}^\star_i$. The <span style="color:blue;">counterfactual</span> obtained with matching $T^\star(\mathbf{x}_{0,i})$ is shown as a <span style="color:blue;">blue square</span>.
#| label: fig-ternary-matching
#| fig-height: 8
#| fig-width: 8
#| warning: false

# Women
tb_plot_females <- 
  as_tibble(X0_glm_1) |> 
  mutate(Gender = "Female", model = "glm_1") |> 
  bind_rows(
    as_tibble(X0_glm_2) |> 
      mutate(Gender = "Female", model = "glm_2")
  ) |> 
  bind_rows(
    as_tibble(X0_rf) |> 
      mutate(Gender = "Female", model = "rf")
  ) |> 
  bind_rows(
    as_tibble(X0_gbm) |> 
      mutate(Gender = "Female", model = "gbm",)
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

# Males individuals, with a column weights_i giving their weight used to 
# construct the counterfactual for indiv i
tb_plot_males <- 
  as_tibble(X1_glm_1) |> 
  mutate(
    Gender = "Male", model = "glm_1", weights_i = weights_i_glm_1
  ) |> 
  bind_rows(
    as_tibble(X1_glm_2) |> 
      mutate(
        Gender = "Male", model = "glm_2", weights_i = weights_i_glm_2
      )
  ) |> 
  bind_rows(
    as_tibble(X1_rf) |> 
      mutate(
        Gender = "Male", model = "rf", weights_i = weights_i_rf
      )
  ) |> 
  bind_rows(
    as_tibble(X1_gbm) |> 
      mutate(
        Gender = "Male", model = "gbm", weights_i = weights_i_rf
      )
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

indiv_i <- 
  as_tibble_row(indiv_i_glm_1) |> mutate(Gender = "Female", model = "glm_1") |> 
  bind_rows(
    as_tibble_row(indiv_i_glm_2) |> mutate(Gender = "Female", model = "glm_2")
  ) |> 
  bind_rows(
    as_tibble_row(indiv_i_rf) |> mutate(Gender = "Female", model = "rf")
  ) |> 
  bind_rows(
    as_tibble_row(indiv_i_gbm) |> mutate(Gender = "Female", model = "gbm")
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

cfact_indiv_i <- 
  as_tibble(cfact_i_glm_1) |> mutate(Gender = "Male", model = "glm_1") |> 
  bind_rows(
    as_tibble(cfact_i_glm_2) |> mutate(Gender = "Male", model = "glm_2")
  ) |> 
  bind_rows(
    as_tibble(cfact_i_rf) |> mutate(Gender = "Male", model = "rf")
  ) |> 
  bind_rows(
    as_tibble(cfact_i_gbm) |> mutate(Gender = "Male", model = "gbm")
  ) |> 
  mutate(
    model = factor(
      model, 
      levels = c("obs", "glm_1", "glm_2", "rf", "gbm"),
      labels = c(
        "Observed Values",
        "GAM-MLR(1)", "GAM-MLR(2)", "Random Forest", 
        "Gradient Boosting Model"
      )
    )
  )

ggtern(
  mapping = aes(x = cars, y = other, z = equipment, colour = Gender)
) +
  geom_point(
    data = tb_plot_females,
    size = .1,
    alpha = .6
  ) +
  geom_point(
    data = tb_plot_males |> 
      group_by(model) |> 
      mutate(high = weights_i > quantile(weights_i, probs = .995)), 
    mapping = aes(size = weights_i, alpha = high),
  ) +
  geom_point(data = indiv_i, size = 3, colour = "white") +
  geom_point(data = indiv_i, size = 2) +
  geom_point(data = cfact_indiv_i, size = 3, colour = "white", shape = 15) +
  geom_point(data = cfact_indiv_i, size = 2, shape = 15) +
  facet_wrap(~model) +
  labs(x = "Cars", y = "Other", z = "Equip.") +
  scale_colour_manual(
    values = c("Female" = "red", "Male" = "blue"), guide = "none"
  ) +
  scale_size_continuous(range = c(0, 2), guide = "none") +
  scale_alpha_discrete(guide = "none") +
  theme_paper() +
  theme(
    tern.axis.arrow.show = TRUE,
    tern.axis.arrow.sep = .13,
    tern.axis.vshift = .05,
  ) +
  theme_hidetitles()
```


To finish, let us look more closely to the i-th woman for which we have shown the counterfactual on the ternary plot. The value of the `Purpose` variable for her is `r as.character(german.credit[ind_0[i], "Purpose"])`:
```{r show-purpose-woman-i}
german.credit[ind_0[i], "Purpose"]
```

Using the GAM-MLR(2) model, we obtained the following composition:
```{r show-composition-woman}
X0_glm_2[i, ]
```

The closest points in the group of men, obtained using [Algorithm 1.2](application-toydataset.qmd#alg-2) are:
```{r define-ind_close_points}
ind_close_points <- tail(order(weights_i_glm_2), 3)
X1_glm_2[ind_close_points, ] |> 
  as_tibble() |> 
  mutate(
    Purpose = german.credit[ind_1[ind_close_points], ] |> select(Purpose),
    index = ind_close_points,
    weights_i = weights_i_glm_2[ind_close_points]) |> 
  arrange(desc(weights_i)) |> 
  kableExtra::kbl(booktabs = TRUE) |> 
  kableExtra::kable_paper()
```
For the first two closest men, $x_{1,j}=$ other. So, it would make sense to suppose that the counterfactual version of woman $i$ with an "other" credit is a man with the same purpose.

The counterfactual version for the composition is:
```{r}
cfact_i_glm_2
```

The counterfactual categorical value would thus be:
```{r}
colnames(cfact_i_glm_2)[which.max(cfact_i_glm_2)]
```

For comparison, using Gaussian OT, the counterfactual would be:
```{r}
transp_glm_2 |> slice(i)
```

